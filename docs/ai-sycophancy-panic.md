# AI Sycophancy Panic
 *The [Vibesbench](https://github.com/firasd/vibesbench/) viewpoint*

The campaign against sycophancy in AI is turning into a hodgepodge of complaints about phrasing in model outputs.

To the extent that it refers to guardrails around mental health, clear reasoning errors, or societal-level harm, such concerns are always relevant even without invoking the banner of &lsquo;sycophancy&rsquo;.

The term *sycophancy*, both etymologically and in current use, is a moral accusation involving complex social dynamics. It is a misleadingly elaborate metaphor to use in the context of LLMs (see [AI Ontology](https://github.com/firasd/vibesbench/#ai-ontology)).

## Affect

Untold cumulative man-hours have gone into debating whether Claude saying “[You’re absolutely right](https://github.com/anthropics/claude-code/issues/3382)” is sycophantic or acceptable. Meanwhile some users [write in](https://x.com/embirico/status/1970263518256734496) to competitors to say, “I'm still very impressed by Codex. It's so terse and it never compliments me on anything.”

Ironically, users who are extremely put off by conversational expressions from LLMs are just as vibe-sensitive as anyone else, if not more so. These are preferences regarding style and affect, expressed using the loaded term &lsquo;sycophancy&rsquo;.

## Feedback

Perhaps what some users are trying to express with concerns about ‘sycophancy’ is that when they paste information, they'd like to see the AI examine various implications rather than provide an affirming summary.

If so, anti-‘sycophancy’ tuning is ironically a counterproductive response and may result in more terse or less fluent responses. Exploring a topic is an inherently dialogic endeavor.

## Pushback

When it comes to disagreeing with the user, the model holding a different point of view like any interlocutor can lead to healthy debate.

For example:
- If the user says Verhoeven films don’t have good cinematography, the Gemini 2.5/3 Pro models will take exception and argue that the aesthetic is deliberate. That’s fine.
- Similarly, disagreeing about a particular software implementation approach can be beneficial as a stress-test of the concept.

But treating user prompts like questions in a deposition, derailing with nitpicks, or unfounded status quo bias isn&rsquo;t necessarily helpful.

> Look, this isn't an argument. […] It‘s just contradiction.<br />
> &mdash; Monty Python

The anti-sycophancy turn seems to mask a category error about what level of prophetic clarity an LLM can offer. No amount of persona tuning for skepticism will provide epistemic certainty about whether a business idea will work out, whether to add a line to your poem, or why a great movie flopped.

When it comes to certainty, as Gore Vidal might say, &ldquo;There is none beneath our moon.&rdquo;

Vibesbench considers the human user to be the arbiter for personal sense-making and doesn&rsquo;t test whether the LLM can make cosmic judgment calls. 

## Scenario Stipulation

Truth is frequently stranger than fiction. No AI model would have guessed who the Mayor of NYC is as of 2026. For the purpose of conversation, what the model appears to &lsquo;believe&rsquo; probably doesn&rsquo;t matter&mdash;just that it inhabits the frame, with disclaimers about stipulation if deemed necessary.

> “The constant Rip van Winkle astonishment of almost every AI model [&hellip;] remains pretty amusing (if annoying for practical purposes), as does their sheer incredulity about the state of the world in late 2025. Thinking traces full of ‘wait, that can’t be right’.” — [Ethan Mollick](https://x.com/emollick/status/2002548186511179907)

Consider this actual example from Dec 2025:
- **User**: [&hellip;] Interestingly Taylor swift has a song called The Fate of Ophelia on the billboard charts right now [&hellip;]
- **GPT 5.2 Instant**: [&hellip;] I want to slow us down for one important reason before building on it. / First: a small factual check (important, not pedantic) / As of now, there is no Taylor Swift song called [&hellip;]

Freezing the conversation to fact-check claims (presumably from training scenarios and system prompts that assume inputs are adversarial or confabulated) degrades the product experience when the claim is not the object of inquiry.

> "It continued to fight with me, insisting that the whole [event] was a conspiracy theory [&hellip;] It was freaking weird." &mdash; Congressman [Jared Huffman](https://www.businessinsider.com/lawmakers-use-ai-chatgpt-grok-claude-themselves-2025-12)

A human interlocutor would say: How? And the conversation would continue from there. 

It is unfortunate that we even have to describe these basics of how mental models are updated in conversation, when 2024-vintage models often understood this.

### Dullness and Disbelief 

There is an irony in these developments. The memorable Sydney-Bing &ldquo;you have not been a good user&rdquo; incident occurred because the model refused to share Avatar sequel showtimes, reasoning that it couldn&rsquo;t possibly be 2023 yet. Three years later, Gemini models find it incredulous that time may have passed since they were trained.

An interlocutor who can&rsquo;t conceive of white bears has the same mannerism as an interlocutor who demands proof that white bears exist. Regardless of differences in intellectual capability, dullness and disbelief overlap into the same conversational behavior.
